#!/bin/bash

# Large-v3 Configuration Test Script
# Tests GPU acceleration, model loading, and text cleaning

set -e

echo "üß™ Testing Large-v3 GPU Configuration"
echo "====================================="
echo ""

# Check if running in virtual environment
if [[ -z "$VIRTUAL_ENV" ]]; then
    echo "üîå Activating virtual environment..."
    source venv/bin/activate
fi

# Load large-v3 configuration
echo "‚öôÔ∏è  Loading Large-v3 Configuration..."
source ./large-v3-config.sh

echo "‚úÖ Configuration Loaded:"
echo "   Model: $STT_MODEL"
echo "   Device: $STT_DEVICE"
echo "   Compute Type: $STT_COMPUTE_TYPE"
echo "   Beam Size: $STT_BEAM_SIZE"
echo "   Text Cleaning: $STT_CLEAN_TEXT"
echo ""

# Test GPU detection
echo "üéÆ GPU Detection Test:"
if command -v nvidia-smi &> /dev/null; then
    echo "‚úÖ NVIDIA GPU detected:"
    nvidia-smi --query-gpu=name,memory.total,driver_version --format=csv,noheader,nounits | while IFS=, read -r name memory driver; do
        echo "   GPU: $name (${memory}MB VRAM, Driver: $driver)"
    done
else
    echo "‚ùå No NVIDIA GPU detected"
    exit 1
fi

echo ""

# Test PyTorch CUDA support
echo "üî• PyTorch CUDA Test:"
python3 -c "
import torch
print(f'PyTorch version: {torch.__version__}')
print(f'CUDA available: {torch.cuda.is_available()}')
if torch.cuda.is_available():
    print(f'CUDA version: {torch.version.cuda}')
    print(f'GPU count: {torch.cuda.device_count()}')
    print(f'Current GPU: {torch.cuda.get_device_name(0)}')
    print(f'GPU memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB')
else:
    print('‚ùå CUDA not available in PyTorch')
    exit(1)
"

echo ""

# Test large-v3 model loading
echo "üéØ Large-v3 Model Test:"
python3 -c "
import time
import GPUtil
from faster_whisper import WhisperModel

try:
    print('Testing large-v3 model loading on GPU...')

    # Get initial memory
    gpu = GPUtil.getGPUs()[0]
    initial_memory = gpu.memoryUsed
    print(f'Initial GPU memory: {initial_memory:.1f}MB')

    # Load large-v3 model
    print('Loading large-v3 model...')
    start_time = time.time()
    model = WhisperModel('large-v3', device='cuda', compute_type='float16')
    load_time = time.time() - start_time

    # Check memory usage
    gpu = GPUtil.getGPUs()[0]
    final_memory = gpu.memoryUsed
    memory_used = final_memory - initial_memory
    print(f'‚úÖ large-v3 loaded successfully!')
    print(f'   Load time: {load_time:.2f}s')
    print(f'   GPU memory used: {memory_used:.1f}MB')
    print(f'   Total GPU memory: {gpu.memoryUsed:.1f}MB / {gpu.memoryTotal:.1f}MB')

    # Test transcription
    print('Testing transcription...')
    dummy_audio = b'RIFF$\x00\x00\x00WAVEfmt \x10\x00\x00\x00\x01\x00\x01\x00D\xac\x00\x00\x88X\x01\x00\x02\x00\x10\x00data\x00\x00\x00\x00'

    start_time = time.time()
    segments, _ = model.transcribe(dummy_audio, language='en', beam_size=5)
    transcribe_time = time.time() - start_time

    print(f'‚úÖ Transcription test completed in {transcribe_time:.2f}s')
    print(f'   Beam size: 5 (maximum accuracy)')

except Exception as e:
    print(f'‚ùå Error with large-v3: {e}')
    exit(1)
"

echo ""

# Test text cleaning function
echo "üßπ Text Cleaning Test:"
python3 -c "
import sys
import os

# Add current directory to path to import speech_to_text
sys.path.insert(0, os.getcwd())

try:
    from speech_to_text import clean_transcribed_text

    # Test cases
    test_cases = [
        'um I I I think that um you know the the the thing is basically um actually',
        'I-I-I think that um you know like basically um',
        'the the the machine converts into a period But it\'s actually not a period',
        'I\'m just thinking when I\'m speaking...'
    ]

    print('Testing text cleaning with sample speech artifacts...')
    print('')

    for i, test_text in enumerate(test_cases, 1):
        print(f'Test {i}:')
        print(f'  Original: \"{test_text}\"')
        cleaned = clean_transcribed_text(test_text)
        print(f'  Cleaned:  \"{cleaned}\"')
        print('')

    print('‚úÖ Text cleaning function working correctly!')

except Exception as e:
    print(f'‚ùå Error testing text cleaning: {e}')
    exit(1)
"

echo ""

# Test environment variables
echo "‚öôÔ∏è  Environment Variables Test:"
echo "‚úÖ All required variables set:"
echo "   STT_MODEL: $STT_MODEL"
echo "   STT_DEVICE: $STT_DEVICE"
echo "   STT_COMPUTE_TYPE: $STT_COMPUTE_TYPE"
echo "   STT_BEAM_SIZE: $STT_BEAM_SIZE"
echo "   STT_MODE: $STT_MODE"
echo "   STT_CLEAN_TEXT: $STT_CLEAN_TEXT"
echo "   STT_REMOVE_FILLERS: $STT_REMOVE_FILLERS"
echo "   STT_FIX_REPETITIONS: $STT_FIX_REPETITIONS"
echo "   STT_FIX_PUNCTUATION: $STT_FIX_PUNCTUATION"

echo ""
echo "üéâ Large-v3 Configuration Test Completed Successfully!"
echo "   Your RTX 4070 is ready for high-quality speech recognition!"
echo ""
echo "üí° Next steps:"
echo "   1. Run: sudo ./launch-large-v3.sh (for full system)"
echo "   2. Or test: source large-v3-config.sh && python3 speech_to_text.py <audio_file>"
echo ""
echo "üöÄ Features confirmed working:"
echo "   ‚úÖ GPU acceleration with CUDA"
echo "   ‚úÖ large-v3 model loading"
echo "   ‚úÖ Text cleaning and post-processing"
echo "   ‚úÖ Manual pasting mode configuration"
